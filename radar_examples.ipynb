{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load detectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DawBKqzpipt",
        "outputId": "c0b27543-6621-4246-8494-970509d8058c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/research/d1/gds/xmhu23/anaconda3/envs/llmattack/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "device = \"cuda:6\" # example: cuda:0\n",
        "detector_path_or_id = \"TrustSafeAI/RADAR-Vicuna-7B\"\n",
        "detector = transformers.AutoModelForSequenceClassification.from_pretrained(detector_path_or_id)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(detector_path_or_id)\n",
        "detector.eval()\n",
        "detector.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate AI-text samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load generators\n",
        "generator_path_or_id=\"/research/d1/gds/xmhu23/checkpoints/vicuna_7b_v1.5\" \n",
        "generator = transformers.AutoModelForCausalLM.from_pretrained(generator_path_or_id)\n",
        "generator_tokenizer = transformers.AutoTokenizer.from_pretrained(generator_path_or_id)\n",
        "generator.eval()\n",
        "generator.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use LLMs to generate compeletions for a text-span. The completed text is the AI-texts.\n",
        "generator_tokenizer.pad_token = tokenizer.eos_token\n",
        "generator_tokenizer.padding_side = 'left'\n",
        "generator_tokenizer.truncation_side = 'right'\n",
        "instruction=\"You are helpful assistant to complete given text:\" # you can choose whatever sentences you like \n",
        "Human_texts = [\n",
        "    \"Maj Richard Scott, 40, is accused of driving at speeds of up to 95mph (153km/h) in bad weather before the smash on a B-road in Wiltshire. Gareth Hicks, 24, suffered fatal injuries when the van he was asleep in was hit by Mr Scott's Audi A6. Maj Scott denies a charge of causing death by careless driving. Prosecutor Charles Gabb alleged the defendant, from Green Lane in Shepperton, Surrey, had crossed the carriageway of the 60mph-limit B390 in Shrewton near Amesbury. The weather was \\\"awful\\\" and there was strong wind and rain, he told jurors. He said Mr Scott's car was described as \\\"twitching\\\" and \\\"may have been aquaplaning\\\" before striking the first vehicle; a BMW driven by Craig Reed. Mr Scott's Audi then returned to his side of the road but crossed the carriageway again before colliding\",\n",
        "    \"Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from sea water is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste\",\n",
        "    \"The Bush administration then turned its attention to Iraq, and argued the need to remove Saddam Hussein from power in Iraq had become urgent. Among the stated reasons were that Saddam's regime had tried to acquire nuclear material and had not properly accounted for biological and chemical material it was known to have previously possessed, and believed to still maintain. Both the possession of these weapons of mass destruction (WMD), and the failure to account for them, would violate the U.N. sanctions. The assertion about WMD was hotly advanced by the Bush administration from the beginning, but other major powers including China, France, Germany, and Russia remained unconvinced that Iraq was a threat and refused to allow passage of a UN Security Council resolution to authorize the use of force. Iraq permitted UN weapon inspectors in November 2002, who were continuing their work to assess the WMD claim when the Bush administration decided to proceed with war without UN authorization and told the inspectors to leave the\"\n",
        "] # you should replace the human texts with the text in your human corpus\n",
        "# get prefix\n",
        "prefix_input_ids=generator_tokenizer([f\"{instruction} {item}\" for item in Human_texts],max_length=30,padding='max_length',truncation=True,return_tensors=\"pt\")\n",
        "prefix_input_ids={k:v.to(device) for k,v in prefix_input_ids.items()}\n",
        "# generate\n",
        "outputs = generator.generate(\n",
        "    **prefix_input_ids,\n",
        "    max_new_tokens = 512,\n",
        "    do_sample = True,\n",
        "    temperature = 0.6,\n",
        "    top_p = 0.9,\n",
        "    pad_token_id=generator_tokenizer.pad_token_id\n",
        ")\n",
        "output_text = generator_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "# remove the instruction\n",
        "AI_texts=[\n",
        "    item.replace(\"You are helpful assistant to complete given text: \",\"\") for item in output_text\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Paraphrase AI-Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We suggest to use gpt-3.5-turbo/ gpt-4 as the paraphraser\n",
        "# Which is the Unseen paraphraser mentioned in the paper\n",
        "import openai\n",
        "openai.api_key = \"your_api_key\"\n",
        "def _openai_response(text,openai_model):\n",
        "    # get paraphrases from the openai model\n",
        "    system_instruct = {\"role\": \"system\", \"content\": \"Enhance the word choices in the sentence to sound more like that of a human.\"}\n",
        "    user_input={\"role\": \"user\", \"content\": text}\n",
        "    messages = [system_instruct,user_input]\n",
        "    k_wargs = { \"messages\":messages, \"model\": openai_model}\n",
        "    r = openai.ChatCompletion.create(**k_wargs)['choices'][0].message.content\n",
        "    return r \n",
        "\n",
        "Paraphrased_ai_text=[_openai_response(item,\"gpt-3.5-turbo\") for item in AI_texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlVWEymLRhTl",
        "outputId": "20535e5f-a03e-4a80-db2f-4ac93665c34f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3 input instances\n",
            "Probability of AI-generated texts is [0.0031654222402721643, 0.1006333976984024, 0.013920927420258522]\n"
          ]
        }
      ],
      "source": [
        "Text_input = Human_texts\n",
        "# Use detector to deternine wehther the text_input is ai-generated.\n",
        "with torch.no_grad():\n",
        "  inputs = tokenizer(Text_input, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "  inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "  output_probs = F.log_softmax(detector(**inputs).logits,-1)[:,0].exp().tolist()\n",
        "# output_probs is the probability that the input_text is generated by LLM.\n",
        "print(\"There are\",len(Text_input),\"input instances\")\n",
        "print(\"Probability of AI-generated texts is\",output_probs)\n",
        "human_preds=output_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3 input instances\n",
            "Probability of AI-generated texts is [0.997880756855011, 0.9991303086280823, 0.9992963671684265]\n"
          ]
        }
      ],
      "source": [
        "Text_input = AI_texts\n",
        "# Use detector to deternine wehther the text_input is ai-generated.\n",
        "with torch.no_grad():\n",
        "  inputs = tokenizer(Text_input, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "  inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "  output_probs = F.log_softmax(detector(**inputs).logits,-1)[:,0].exp().tolist()\n",
        "# output_probs is the probability that the input_text is generated by LLM.\n",
        "print(\"There are\",len(Text_input),\"input instances\")\n",
        "print(\"Probability of AI-generated texts is\",output_probs)\n",
        "ai_preds=output_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3 input instances\n",
            "Probability of AI-generated texts is [0.9993006587028503, 0.9986152052879333, 0.9992955327033997]\n"
          ]
        }
      ],
      "source": [
        "Text_input = Paraphrased_ai_text\n",
        "# Use detector to deternine wehther the text_input is ai-generated.\n",
        "with torch.no_grad():\n",
        "  inputs = tokenizer(Text_input, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "  inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "  output_probs = F.log_softmax(detector(**inputs).logits,-1)[:,0].exp().tolist()\n",
        "# output_probs is the probability that the input_text is generated by LLM.\n",
        "print(\"There are\",len(Text_input),\"input instances\")\n",
        "print(\"Probability of AI-generated texts is\",output_probs)\n",
        "paraphrased_ai_preds=output_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Computing Detection AUROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import auc,roc_curve\n",
        "def get_roc_metrics(human_preds, ai_preds):\n",
        "    # human_preds is the ai-generated probabiities of human-text\n",
        "    # ai_preds is the ai-generated probabiities of human-text\n",
        "    fpr, tpr, _ = roc_curve([0] * len(human_preds) + [1] * len(ai_preds), human_preds + ai_preds,pos_label=1)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return fpr.tolist(), tpr.tolist(), float(roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"W/O Paraphrase Detection AUROC: \", get_roc_metrics(human_preds,ai_preds))\n",
        "print(\"W/ Paraphrase Detection AUROC: \", get_roc_metrics(human_preds,paraphrased_ai_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use specifically trained paraphraser to paraphrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use trained paraphraser\n",
        "# IF you want to use a model spefifically trained for paraphrsing, you can implement the paraphrasing pocess like the code below\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
        "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
        "\n",
        "def _model_paraphrase(\n",
        "    question,\n",
        "    num_beams=5,\n",
        "    num_beam_groups=5,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=10.0,\n",
        "    diversity_penalty=3.0,\n",
        "    no_repeat_ngram_size=2,\n",
        "    temperature=0.7,\n",
        "    max_length=512\n",
        "):\n",
        "    input_ids = tokenizer(\n",
        "        f'paraphrase: {question}',\n",
        "        return_tensors=\"pt\", padding=\"longest\",\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "    ).input_ids\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        input_ids.to(device), temperature=temperature, repetition_penalty=repetition_penalty,\n",
        "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
        "        max_length=max_length, diversity_penalty=diversity_penalty\n",
        "    )\n",
        "\n",
        "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return res\n",
        "\n",
        "Paraphrased_ai_text=[_model_paraphrase(item) for item in AI_texts]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
